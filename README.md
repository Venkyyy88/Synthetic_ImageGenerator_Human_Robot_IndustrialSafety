# Synthetic ImageÂ Generator

> **Generate richlyâ€‘annotated synthetic datasets for industrial safety applications â€” fully scripted with BlenderÂ 3.5 + BlenderProc.**

---

## TableÂ ofÂ Contents
1. [Overview](#overview)  
2. [Features](#features)  
3. [Directory Structure](#directory-structure)  
4. [QuickÂ Start](#quick-start)  
5. [Configuration](#configuration)  
6. [Usage](#usage)  
7. [CodeÂ Architecture](#code-architecture)  
8. [ExtendingÂ theÂ Pipeline](#extending-the-pipeline)  
9. [Roadmap](#roadmap)  
10. [Contributing](#contributing)  
11. [LicenseÂ &Â Acknowledgements](#license--acknowledgements)

---

## Overview
`ImageÂ Generator` procedurally assembles 3â€‘D scenes containing a **Frankaâ€‘style Panda robot**, interchangeable **endâ€‘effectors**, a **workpiece**, a **worktable**, and a **human worker**.  
The engine randomises object poses, camera placement, lighting, textures, and materials, then renders:

* Highâ€‘resolution **RGB images**
* **COCOâ€‘style instance annotations** (JSON)
* Perâ€‘pixel **HDF5 segmentation masks**

All assets are ready for downstream computerâ€‘vision tasks such as safetyâ€‘zone monitoring or graspâ€‘pose estimation.

---

## Features
| Category | Capability |
|----------|------------|
| **Scene Generation** | Procedural placement of workpiece & robot; human worker pose control (IK) |
| **Robotic Arm** | URDFâ€‘driven or native *.blend* kinematics; perâ€‘axis limit sampling based on Franka Emika datasheet |
| **Safety** | Runtime visual safety sphere (configurable radius) attached to endâ€‘effector |
| **Photorealism** | Randomised HDRI lighting, camera intrinsics/extrinsics, physicallyâ€‘based materials |
| **Outputs** | RGB (JPEGÂ /Â PNG), COCO JSON, HDF5 masks & normals |
| **Batch Mode** | Oneâ€‘click dataset generation via `image_gen.bat` |

---

## Directory Structure
```
BlenderProc/
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ image_gen.py              # Main automation script (see CodeÂ Architecture)
â”‚   â”œâ”€â”€ image_gen_config.yaml     # Userâ€‘configurable parameters (sample below)
â”‚   â”œâ”€â”€ image_gen.bat             # Batch rendering helper (Windows)
â”‚   â””â”€â”€ output/                   # Autoâ€‘generated images & annotations
â”‚       â”œâ”€â”€ rgb/
â”‚       â”œâ”€â”€ hdf5/
â”‚       â””â”€â”€ coco_annotations.json
â”œâ”€â”€ resources/
â”‚   â””â”€â”€ Dataset/                  # 3â€‘D asset library
â”‚       â”œâ”€â”€ scene.blend           # Static scene layout (table, lights, â€¦)
â”‚       â”œâ”€â”€ panda.blend           # Frankaâ€‘style robot arm
â”‚       â””â”€â”€ worker.blend          # Human worker mesh + armature
â””â”€â”€ README.md                     #Â You are here ðŸš€
```

---

## QuickÂ Start
### 1Â â€”Â Install Dependencies (Windows)
```bash
# BlenderÂ 3.5 â€“ required for BlenderProc â‰¥Â 2.4
choco install blender --version 3.5.0   # or download manually

# BlenderProc (pip inside ANY Python â‰¥3.8)
pip install blenderproc
```

### 2Â â€”Â Clone & Render
```bash
git clone <YOURâ€‘FORKâ€‘URL>
cd imageâ€‘generator/blenderproc/code

# First render (writes to ./output/test_final)
blenderproc run image_gen.py --pipeline_config image_gen_config.yaml
```
If everything is configured correctly, the script will create one RGB image, its COCO annotation, and an HDF5 segmentation mask.

---

## Configuration
All runtime parameters live in **`code/image_gen_config.yaml`**. A fullyâ€‘annotated default file is available in the given path.

### Parameter Reference
| Key | Description |
|-----|-------------|
| `scene_path` / `panda_path` / `worker_path` | Relative or absolute paths to the *.blend* assets |
| `num_images` | Frames per execution |
| `camera_lens`, `img_width`, `img_height` | Camera intrinsics & output resolution |
| `light_energy`, `light_type` | Physicallyâ€‘based lamp setup |
| `bg_color_rgb` | Background gradient â€” grey by default |
| `safety_zone_radius` | Radius (m) of translucent red sphere around Axisâ€‘7 |
| `bones_to_randomize*` | Joint limits applied per render (rad) |
| `category_ids` | COCO class mapping written into JSON |  


---

## Usage
### Single Run
```bash
blenderproc run code/image_gen.py --pipeline_config code/image_gen_config.yaml
```

### Batch Rendering
```bash
# Render with different seeds / configs (Windows helper)
code\image_gen.bat
```

### Visualise Outputs
BlenderProc ships with two convenient CLI helpers for inspecting what you rendered:
```bash
# COCO overlay for image indexÂ 50
blenderproc vis coco -b code/output/test_final -i 50 -c coco_annotations.json

# Inspect segmentation HDF5
blenderproc vis hdf5 code/output/test_final/hdf5/69.hdf5
```

#### Sample OutputÂ Gallery
Below is a quick glance at what a single frame looks like after postâ€‘processing.

<p align="center">
  <!-- RGB -->
  <img src="docs/images/rgb_sample.png" alt="RGB frame"   width="30%" />
  <!-- segmentation -->
  <img src="docs/images/segmap_sample.png" alt="Segmentation mask" width="30%" />
  <!-- boundingâ€‘boxes -->
  <img src="docs/images/bbox_sample.png" alt="Boundingâ€‘box overlay" width="30%" />
</p>

*LeftÂ â†’Â Right:* **RGB**, **Segmentation**, **Bounding Boxes**

---

## CodeÂ Architecture
| File | Responsibility |
|------|----------------|
| **image_gen.py** | Endâ€‘toâ€‘end pipeline: load config â†’ set up scene â†’ randomise objects & bones â†’ render â†’ write outputs |
| `load_config()` | Safe YAML loader |
| `randomize_workpiece_on_table()` | Uniform xy pose + z offset based on mesh height |
| `randomize_panda_armature_poses()` | Applies perâ€‘axis limits from `bones_to_randomize` and updates safety sphere |
| `set_random_armature_transform_near_table()` | Places worker mesh and calls `randomize_arm_positions()` |
| `configure_camera_and_lighting()` | Spherical shell sampler around table centroid; attaches area light |
| `render_scene()` | Iterates `num_images`, resets keyframes, triggers COCOÂ +Â HDF5 writer |
| `assign_category_ids()` | Encodes `category_ids` onto every Blender object for COCO export |

The script is intentionally selfâ€‘containedâ€”no addâ€‘ons or external modules beyond BlenderProc.

---

## ExtendingÂ theÂ Pipeline
| Goal | HowÂ to |
|------|--------|
| **Add new endâ€‘effectors** | Import your textured mesh into `panda.blend` or reference via `bproc.loader.load_blend()` and update `category_ids`. |
| **Physics collisions** | Inject collision checks in `utils/collision.py` (TODO) and call inside `randomize_*()` functions. |
| **Advanced worker animation** | Replace Euler sampling with IK constraints or BVH mocap; update `bones_to_randomize_worker`. |
| **Domain randomisation sweeps** | Wrap `blenderproc run` in a shell script to loop over YAML presets & seeds. |

---

## Roadmap
- [ ] IKâ€‘based worker motion with strict joint limits  
- [ ] Realâ€‘time collision avoidance (table â†” robot â†” worker)  
- [ ] Expanded material & texture randomisation library  
- [ ] DockerÂ +Â CLI wrapper for headless cloud rendering  

Contributions and feature requests are welcome!

---

## Contributing
1. Fork ðŸ’«  
2. Create feature branch: `git checkout -b feat/awesome`  
3. Commit + push: `git commit -m "feat: add awesome" && git push`  
4. Open a Pull Request & tag a maintainer.

We enforce `preâ€‘commit`, `black`, and `ruff`â€”please ensure hooks pass before opening a PR.

---

## LicenseÂ &Â Acknowledgements

This project is released under the **MITÂ License**.  
Robotic arm & worker assets Â©â€¯TurboSquid â€” used with commercial licence.  
BlenderÂ®Â and BlenderProcâ„¢ are trademarks of their respective owners and are referenced here solely to describe compatibility.
